{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 4\n",
    "\n",
    "Use this notebook as a starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data:\n",
    "\n",
    "- https://github.com/gastonstat/CreditScoring\n",
    "- Also available [here](https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-06-trees/CreditScoring.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-03 22:17:41--  https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-06-trees/CreditScoring.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 182489 (178K) [text/plain]\n",
      "Saving to: ‘CreditScoring.csv’\n",
      "\n",
      "CreditScoring.csv   100%[===================>] 178.21K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2021-10-03 22:17:41 (5.89 MB/s) - ‘CreditScoring.csv’ saved [182489/182489]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-06-trees/CreditScoring.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation \n",
    "\n",
    "We'll talk about this dataset in more details in week 6. But for now, use the following code to get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('CreditScoring.csv')\n",
    "df.columns = df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the features are encoded as numbers. Use the following code to de-code them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_values = {\n",
    "    1: 'ok',\n",
    "    2: 'default',\n",
    "    0: 'unk'\n",
    "}\n",
    "\n",
    "df.status = df.status.map(status_values)\n",
    "\n",
    "\n",
    "home_values = {\n",
    "    1: 'rent',\n",
    "    2: 'owner',\n",
    "    3: 'private',\n",
    "    4: 'ignore',\n",
    "    5: 'parents',\n",
    "    6: 'other',\n",
    "    0: 'unk'\n",
    "}\n",
    "\n",
    "df.home = df.home.map(home_values)\n",
    "\n",
    "marital_values = {\n",
    "    1: 'single',\n",
    "    2: 'married',\n",
    "    3: 'widow',\n",
    "    4: 'separated',\n",
    "    5: 'divorced',\n",
    "    0: 'unk'\n",
    "}\n",
    "\n",
    "df.marital = df.marital.map(marital_values)\n",
    "\n",
    "records_values = {\n",
    "    1: 'no',\n",
    "    2: 'yes',\n",
    "    0: 'unk'\n",
    "}\n",
    "\n",
    "df.records = df.records.map(records_values)\n",
    "\n",
    "job_values = {\n",
    "    1: 'fixed',\n",
    "    2: 'partime',\n",
    "    3: 'freelance',\n",
    "    4: 'others',\n",
    "    0: 'unk'\n",
    "}\n",
    "\n",
    "df.job = df.job.map(job_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the numerical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['income', 'assets', 'debt']:\n",
    "    df[c] = df[c].replace(to_replace=99999999, value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove clients with unknown default status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.status != 'unk'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['default'] = (df.status == 'default').astype(int)\n",
    "del df['status']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the categorical variables? What are the numerical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['home', 'marital', 'records', 'job']\n",
      "['seniority', 'time', 'age', 'expenses', 'income', 'assets', 'debt', 'amount', 'price']\n"
     ]
    }
   ],
   "source": [
    "df.head().T\n",
    "strings = list(df.dtypes[df.dtypes == 'object'].index)\n",
    "print(strings)\n",
    "numerical = [c for c in df.columns if c not in strings and c != 'default']\n",
    "print(numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into 3 parts: train/validation/test with 60%/20%/20% distribution. Use `train_test_split` funciton for that with `random_state=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2672 891 891\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_val, test = train_test_split(df, test_size=0.2, random_state=1)\n",
    "train, val = train_test_split(train_val, test_size=0.25, random_state=1)\n",
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "ROC AUC could also be used to evaluate feature importance of numerical variables. \n",
    "\n",
    "Let's do that\n",
    "\n",
    "* For each numerical variable, use it as score and compute AUC with the \"default\" variable\n",
    "* Use the training dataset for that\n",
    "\n",
    "\n",
    "If your AUC is < 0.5, invert this variable by putting \"-\" in front\n",
    "\n",
    "(e.g. `-df_train['expenses']`)\n",
    "\n",
    "AUC can go below 0.5 if the variable is negatively correlated with the target varialble. You can change the direction of the correlation by negating this variable - then negative correlation becomes positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('expenses', 0.5009184217217011), ('price', 0.5043329862114843), ('debt', 0.5047829675783548), ('time', 0.5608662489595051), ('age', 0.5732933272499939), ('amount', 0.5910773431595518), ('assets', 0.6486042567122802), ('income', 0.682006666132633), ('seniority', 0.7093778624491943)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "auc_scores = []\n",
    "for numerical_var in numerical:\n",
    "    auc_score = roc_auc_score(train[\"default\"], train[numerical_var])\n",
    "    if auc_score < 0.5:\n",
    "        auc_score = roc_auc_score(train[\"default\"], -train[numerical_var])\n",
    "    auc_scores.append((numerical_var, auc_score))\n",
    "    \n",
    "print(sorted(auc_scores, key=lambda x : x [1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which numerical variable (among the following 4) has the highest AUC?\n",
    "\n",
    "- seniority\n",
    "- time\n",
    "- income\n",
    "- debt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "From now on, use these columns only:\n",
    "\n",
    "```\n",
    "['seniority', 'income', 'assets', 'records', 'job', 'home']\n",
    "```\n",
    "\n",
    "Apply one-hot-encoding using `DictVectorizer` and train the logistic regression with these parameters:\n",
    "\n",
    "```\n",
    "LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0e+00 0.0e+00 0.0e+00 ... 1.0e+00 0.0e+00 1.2e+01]\n",
      " [0.0e+00 0.0e+00 1.0e+00 ... 1.0e+00 0.0e+00 4.0e+00]\n",
      " [6.0e+03 0.0e+00 0.0e+00 ... 1.0e+00 0.0e+00 0.0e+00]\n",
      " ...\n",
      " [0.0e+00 0.0e+00 0.0e+00 ... 0.0e+00 1.0e+00 9.0e+00]\n",
      " [3.0e+03 0.0e+00 0.0e+00 ... 1.0e+00 0.0e+00 0.0e+00]\n",
      " [0.0e+00 0.0e+00 0.0e+00 ... 1.0e+00 0.0e+00 8.0e+00]]\n",
      "[1.96864314e-01 1.17566872e-01 2.69819055e-01 1.23272338e-01\n",
      " 4.14165672e-02 6.03395745e-01 8.47304883e-01 2.30828468e-01\n",
      " 9.20154388e-01 3.40258877e-01 3.19615976e-01 6.28632920e-02\n",
      " 4.05809735e-01 4.06877803e-01 1.01942248e-01 7.06995276e-02\n",
      " 3.02017038e-02 2.74913047e-01 7.92667940e-01 5.23974453e-02\n",
      " 4.90722153e-01 1.33305263e-01 7.04823910e-02 1.78498053e-02\n",
      " 4.18152408e-01 5.33852409e-01 5.31609105e-01 1.02859953e-01\n",
      " 4.30786585e-01 2.42858519e-02 2.25437343e-01 4.93559218e-02\n",
      " 3.15445298e-01 1.23847857e-01 6.97466687e-01 8.41052013e-02\n",
      " 5.28889504e-02 6.76631171e-01 1.11294103e-01 1.21434289e-01\n",
      " 3.95984726e-01 1.00225343e-01 1.12215659e-01 1.76622232e-01\n",
      " 1.22913744e-01 5.37205801e-01 7.96486343e-01 6.94629910e-01\n",
      " 6.10214079e-02 2.27408164e-02 3.36418371e-01 1.06107930e-01\n",
      " 8.43867552e-02 5.33933800e-01 1.53052277e-01 5.56197895e-01\n",
      " 7.71534142e-01 4.06782627e-01 2.82628427e-01 3.35453241e-02\n",
      " 1.19186019e-01 6.07027168e-01 1.44635682e-01 6.16596580e-01\n",
      " 2.65230922e-01 6.24510217e-02 3.23247749e-01 3.82506128e-01\n",
      " 2.24728169e-01 7.50813881e-02 1.46563620e-02 9.09850765e-01\n",
      " 3.39451931e-01 3.95025001e-01 2.54644106e-01 3.75511458e-01\n",
      " 1.86521051e-01 9.29944883e-02 1.11444414e-01 1.95105717e-01\n",
      " 2.94806428e-01 2.48640938e-02 1.98344569e-01 5.47523032e-01\n",
      " 1.23972614e-01 2.31619038e-01 3.57590363e-01 2.05073766e-01\n",
      " 4.87741528e-01 6.98883598e-02 6.02518620e-01 3.50126084e-01\n",
      " 3.21467304e-01 1.28092512e-01 1.53228910e-01 3.90710536e-01\n",
      " 5.96971789e-01 2.01022034e-01 3.80635462e-02 6.03395745e-01\n",
      " 8.83212106e-01 7.95726952e-01 2.32828214e-01 3.90849172e-02\n",
      " 5.53098351e-01 1.70657360e-01 7.79766722e-01 3.30005464e-01\n",
      " 5.63419727e-01 2.47774728e-01 4.36807294e-02 9.25821558e-02\n",
      " 1.99887972e-01 2.07162006e-01 1.37947038e-01 4.58402130e-01\n",
      " 2.97500176e-01 2.46480959e-01 6.35661782e-01 1.41813162e-01\n",
      " 7.70619229e-02 6.50007397e-02 1.41249832e-01 4.62307447e-01\n",
      " 2.47047039e-01 1.67992519e-02 2.31762968e-01 1.53622502e-01\n",
      " 1.67379797e-01 5.13691760e-01 5.89293694e-02 6.19870933e-01\n",
      " 1.09455902e-01 1.98243108e-02 4.38831127e-01 1.17231188e-01\n",
      " 1.87021712e-01 2.00260772e-01 3.97199041e-02 1.44124367e-01\n",
      " 8.48197178e-02 3.51313906e-01 3.65317496e-01 1.26213392e-01\n",
      " 4.22881723e-02 2.84698345e-01 7.11146508e-02 2.93322893e-01\n",
      " 2.95536177e-01 1.01683916e-01 4.88273538e-01 9.07119237e-02\n",
      " 1.74430347e-01 7.99000839e-02 2.41641900e-01 1.03685215e-01\n",
      " 3.90730791e-01 6.82147917e-01 1.45894919e-01 1.08651279e-01\n",
      " 1.51998056e-01 3.34695694e-01 4.27237787e-01 2.45783545e-01\n",
      " 7.98161781e-02 5.73063584e-01 4.76092619e-01 1.03202802e-01\n",
      " 2.55596087e-01 2.13271683e-01 6.07203561e-01 4.45229802e-01\n",
      " 4.06180965e-01 6.32270333e-01 1.15916215e-01 6.41630759e-01\n",
      " 2.03517992e-01 2.01216081e-01 6.10311963e-01 1.96387154e-01\n",
      " 3.84207165e-03 1.34170406e-01 2.83031485e-02 1.70296045e-01\n",
      " 2.48009756e-01 1.75830801e-01 8.17023176e-02 4.69870752e-02\n",
      " 6.74270009e-02 7.12980889e-02 6.96563061e-02 2.13086255e-01\n",
      " 2.31813808e-01 4.93888326e-01 1.18945305e-01 4.12699068e-01\n",
      " 5.12251672e-01 1.57876638e-01 7.43791154e-02 2.26372819e-01\n",
      " 6.36901885e-01 6.68093784e-02 1.22524612e-01 3.84627659e-01\n",
      " 2.90313047e-02 5.20940885e-01 2.43172225e-01 2.27948048e-01\n",
      " 3.28771962e-01 1.77909734e-02 8.29124481e-02 2.07851675e-01\n",
      " 1.41924863e-01 8.26434425e-01 3.46053890e-01 4.04508964e-01\n",
      " 3.20663710e-01 4.11669571e-01 4.79951784e-01 1.62655494e-01\n",
      " 9.08816427e-02 1.91317434e-01 9.20499852e-01 6.51932340e-02\n",
      " 3.76854405e-02 3.69556408e-01 3.69487201e-01 7.02902052e-01\n",
      " 6.10322757e-01 6.41163092e-02 8.76563122e-02 2.06499293e-01\n",
      " 9.25753653e-01 7.62272359e-01 3.44198403e-02 1.07872926e-01\n",
      " 5.77008459e-01 6.93980184e-01 7.49853067e-01 8.00775353e-02\n",
      " 4.11889179e-01 2.26842425e-01 2.55208881e-01 6.17716600e-01\n",
      " 1.94160517e-01 5.60170590e-02 4.51037144e-01 5.34684808e-02\n",
      " 6.22760151e-01 3.22718198e-01 4.45358723e-02 3.76374252e-01\n",
      " 3.90284052e-01 9.98876533e-02 1.81384642e-01 4.50885913e-02\n",
      " 7.48514090e-01 1.26341779e-01 2.34306030e-01 3.41640197e-01\n",
      " 3.13787786e-01 1.19746056e-03 2.16316246e-01 3.54246636e-01\n",
      " 4.23496049e-02 5.40752859e-01 5.69020764e-01 5.32189717e-02\n",
      " 2.78848028e-01 4.93313659e-02 3.84131773e-01 1.74696478e-01\n",
      " 1.01759606e-01 6.45189125e-01 2.68863404e-02 8.96427727e-01\n",
      " 5.22800139e-02 2.07120362e-01 7.38165056e-01 1.50114238e-01\n",
      " 1.55019137e-01 8.25560397e-02 6.34420796e-02 3.86753586e-01\n",
      " 1.32740821e-01 5.19000015e-01 1.48195925e-01 3.45481076e-02\n",
      " 4.64600070e-01 1.16184581e-01 1.57637799e-01 4.54878609e-01\n",
      " 3.75302456e-01 2.43760848e-01 2.96476996e-01 2.49113913e-01\n",
      " 1.78220274e-01 3.52359456e-01 2.66911958e-02 1.47173261e-01\n",
      " 1.31939348e-01 8.38405338e-01 1.67480407e-01 4.71305379e-01\n",
      " 1.94550996e-01 6.99786148e-01 1.30679150e-01 7.63830898e-02\n",
      " 2.03259487e-01 1.75763275e-01 5.43662009e-01 6.81432991e-01\n",
      " 2.89640094e-02 7.14027757e-01 8.75567665e-01 6.83986933e-01\n",
      " 3.89969117e-01 8.85154508e-02 6.99786148e-01 3.44069115e-01\n",
      " 2.71777184e-01 6.49891032e-01 3.44500881e-02 9.04877019e-01\n",
      " 1.33176546e-02 3.92260150e-01 2.90338511e-01 6.32900760e-01\n",
      " 3.74895058e-02 2.85528349e-01 3.63202296e-01 8.65975610e-01\n",
      " 7.42355062e-01 6.53210968e-01 2.89248619e-01 9.85132861e-02\n",
      " 1.63601317e-01 2.71360588e-01 6.54894810e-01 6.53460121e-01\n",
      " 3.76633737e-02 5.40672805e-02 3.53031315e-01 2.31306143e-01\n",
      " 1.96155375e-01 6.25430374e-02 2.03259487e-01 2.01743599e-01\n",
      " 3.95743308e-01 1.10596513e-01 9.13679247e-02 2.18120244e-02\n",
      " 3.99262313e-01 3.35504372e-01 8.18388498e-01 1.85758099e-01\n",
      " 7.65329124e-01 1.19449806e-03 4.04795283e-01 3.15583349e-01\n",
      " 3.33746005e-01 6.63222761e-02 3.63989135e-01 3.88567338e-01\n",
      " 5.23894211e-01 1.77491999e-02 2.38690196e-01 1.54548074e-02\n",
      " 1.01295457e-01 7.89162945e-01 4.31011940e-03 3.73099578e-01\n",
      " 3.07662699e-01 6.37530920e-02 1.12133376e-01 3.44759649e-01\n",
      " 2.14851243e-01 5.05447632e-01 1.45487186e-02 9.00084757e-02\n",
      " 3.39951151e-01 6.36917394e-02 7.52585336e-02 5.27477590e-02\n",
      " 3.37151514e-01 6.93730160e-01 1.40492827e-01 6.98795420e-01\n",
      " 9.84154249e-02 1.19303047e-02 8.24735661e-01 3.55613251e-01\n",
      " 2.92365687e-01 1.56683698e-01 1.13449676e-01 1.14305593e-01\n",
      " 1.62360373e-01 6.92397642e-01 2.26014747e-02 1.14245651e-01\n",
      " 4.81568700e-02 2.36276711e-02 1.44481490e-01 5.97487182e-01\n",
      " 4.41971928e-01 1.23316404e-01 2.34812157e-01 2.78455666e-01\n",
      " 8.03268832e-03 7.49853067e-01 5.03904792e-01 2.03259487e-01\n",
      " 7.75567278e-02 2.50618720e-01 3.54176981e-01 6.07198676e-01\n",
      " 2.05730502e-02 2.43530089e-03 2.07786712e-01 5.13484825e-01\n",
      " 4.17030235e-02 4.58402130e-01 4.30300018e-01 2.95889355e-03\n",
      " 3.22294782e-01 3.76883232e-01 1.85778339e-01 2.37480106e-01\n",
      " 2.88855585e-01 2.33349158e-01 2.61238061e-01 2.23660401e-01\n",
      " 1.29639480e-01 3.71141793e-01 6.99786148e-01 7.74964843e-01\n",
      " 5.44274694e-01 5.47703053e-01 3.86981742e-02 5.01897604e-01\n",
      " 1.12135197e-01 1.50433228e-01 2.59417635e-01 1.00652681e-01\n",
      " 1.10083247e-01 6.17890305e-01 2.69819055e-01 2.03664999e-01\n",
      " 1.52946624e-01 4.20544520e-01 5.66317519e-01 5.95066733e-01\n",
      " 7.99874295e-01 2.30719419e-01 1.53892030e-01 3.96272144e-01\n",
      " 5.30616645e-02 1.17770134e-01 4.97744326e-02 1.46224788e-01\n",
      " 4.84816871e-02 2.92537093e-01 4.81790226e-01 4.30955972e-01\n",
      " 3.67845579e-02 1.54536322e-01 8.45411776e-02 2.61241796e-01\n",
      " 2.64488941e-01 1.47450986e-01 1.84021377e-01 1.48179135e-01\n",
      " 1.09370396e-01 8.45667589e-01 8.21649494e-01 3.76735628e-01\n",
      " 5.35034791e-02 5.66477199e-02 3.71653704e-01 1.78817457e-01\n",
      " 3.40013746e-02 4.07163518e-01 3.61089221e-01 3.89473577e-02\n",
      " 8.19130065e-02 1.03701824e-01 1.57602444e-01 2.74102062e-01\n",
      " 3.71579974e-01 5.83809364e-01 7.19737357e-01 1.48982092e-02\n",
      " 9.33146111e-01 1.64849782e-01 1.85132878e-02 1.00102882e-01\n",
      " 1.66205074e-02 1.79425525e-01 2.81800334e-01 5.11498561e-02\n",
      " 5.30288259e-01 1.17651651e-01 6.32562848e-01 6.25748548e-01\n",
      " 8.31809718e-01 1.97527236e-01 2.62872244e-02 7.65985387e-01\n",
      " 5.74141338e-02 2.95840428e-01 6.45092466e-01 1.29974028e-01\n",
      " 5.50993187e-02 4.84032812e-01 1.34162131e-01 1.56840182e-01\n",
      " 3.60783570e-01 6.27784200e-01 1.91177199e-01 1.67202734e-01\n",
      " 6.99786148e-01 1.16583571e-01 7.04350121e-02 3.83423703e-01\n",
      " 6.41346347e-01 1.66258713e-01 6.84380663e-01 2.92381813e-01\n",
      " 3.60409862e-02 2.66201983e-01 1.46959961e-01 4.38563941e-01\n",
      " 1.63874971e-02 8.57418115e-04 1.15693363e-01 4.09235634e-01\n",
      " 4.86371995e-01 8.36700698e-01 7.43291242e-01 8.19718291e-01\n",
      " 2.51585405e-01 3.41524927e-01 2.08067896e-02 6.57040237e-02\n",
      " 1.01224544e-01 1.43613151e-01 1.12141173e-01 7.60488175e-01\n",
      " 1.67870117e-02 8.20256947e-02 3.54178726e-01 1.43878215e-01\n",
      " 1.52190476e-01 1.56216817e-01 2.11989531e-01 2.39696250e-01\n",
      " 1.11387686e-01 7.27551099e-02 1.42868334e-01 5.60736117e-02\n",
      " 2.04820500e-01 3.11089325e-02 6.94150025e-01 3.92219462e-01\n",
      " 9.14826742e-02 6.96813765e-01 2.36893952e-01 4.73247150e-01\n",
      " 8.43550957e-02 5.78013350e-01 2.06262058e-01 2.77853275e-01\n",
      " 2.65180323e-01 1.81174948e-01 1.47717593e-01 1.21131567e-01\n",
      " 1.36064292e-01 1.65903675e-01 2.68949160e-02 1.57677708e-01\n",
      " 1.81327012e-01 3.38019847e-01 8.07741758e-02 1.36196382e-01\n",
      " 1.21536689e-01 5.00859418e-01 2.16488974e-01 1.23381951e-01\n",
      " 6.89209345e-02 6.55116381e-01 2.71993458e-01 3.83387118e-01\n",
      " 7.33759322e-02 8.19861625e-02 2.16521056e-01 1.27990815e-01\n",
      " 1.43685839e-02 1.08032464e-01 1.10103883e-01 5.63823062e-01\n",
      " 5.40752859e-01 3.92601798e-01 3.73349160e-02 3.94446399e-01\n",
      " 1.57677267e-02 2.80723374e-01 3.31881343e-01 2.57696322e-01\n",
      " 6.88829762e-01 2.81800334e-01 4.68423577e-01 2.00353144e-01\n",
      " 5.68827858e-01 9.00774823e-02 2.78646349e-02 3.79714170e-01\n",
      " 1.61159693e-01 1.00584432e-01 1.45825892e-01 9.37762801e-01\n",
      " 1.62766348e-01 1.25771792e-01 2.63916850e-01 4.39240693e-01\n",
      " 3.04282571e-01 2.84698345e-01 4.09506507e-02 1.39183700e-01\n",
      " 3.04623755e-01 5.30713657e-02 5.37244276e-01 3.11565425e-02\n",
      " 3.98819398e-01 1.80746206e-01 1.61179766e-01 4.47408427e-01\n",
      " 2.63040707e-01 6.83430655e-01 2.67946330e-01 5.67864984e-01\n",
      " 6.59802835e-01 5.03471548e-01 3.27446623e-01 3.42418649e-02\n",
      " 1.22743684e-01 5.25791525e-02 6.23426262e-01 3.84328843e-01\n",
      " 4.13072219e-01 4.05756553e-01 3.29094444e-01 9.45070222e-02\n",
      " 1.75070432e-01 7.82285061e-01 6.99036319e-02 2.11655790e-01\n",
      " 5.54129020e-01 3.05312655e-01 2.22253663e-01 3.23505272e-01\n",
      " 1.32042045e-01 4.28093389e-01 1.11973948e-01 1.29671801e-01\n",
      " 5.59679931e-01 6.90185730e-01 2.91819312e-02 1.28889497e-01\n",
      " 5.13005505e-01 8.89575412e-01 4.03714606e-01 2.65472733e-01\n",
      " 2.09299210e-02 4.11368692e-02 3.42277028e-01 2.54122670e-01\n",
      " 9.65341672e-02 5.35482728e-01 8.39263645e-02 7.24274245e-02\n",
      " 2.13353235e-01 3.25842624e-01 7.17761772e-01 1.44276900e-01\n",
      " 1.21478460e-01 9.53211790e-02 9.38432556e-02 4.63572868e-01\n",
      " 1.23830416e-01 1.32097784e-01 1.82964796e-01 2.79138025e-01\n",
      " 5.96537734e-01 3.07829044e-01 4.70790166e-02 2.33438267e-01\n",
      " 7.45189900e-02 1.78220274e-01 2.72328034e-01 4.23005206e-01\n",
      " 1.09483058e-01 2.53435336e-01 2.17123380e-01 4.86335312e-02\n",
      " 3.91404550e-01 5.53896227e-01 2.63521501e-01 3.10836884e-02\n",
      " 6.20396964e-02 5.48553869e-03 6.36873005e-01 5.53698026e-02\n",
      " 3.03343646e-01 5.73296013e-01 4.29175421e-01 8.29192712e-02\n",
      " 1.94200784e-01 1.45515824e-01 3.65925726e-01 3.09219251e-01\n",
      " 5.40752859e-01 1.27752937e-01 3.18328918e-02 5.32862208e-01\n",
      " 5.66863343e-01 1.72085893e-01 4.63283293e-01 1.97386852e-01\n",
      " 3.02511696e-01 1.02029289e-01 3.59440722e-01 2.23765444e-01\n",
      " 7.00330828e-02 5.00241841e-01 1.67118110e-01 3.15616562e-01\n",
      " 1.34986020e-01 4.49049066e-01 5.78229010e-01 9.94811106e-02\n",
      " 2.50090623e-01 5.03556267e-02 7.25004894e-02 3.15833070e-02\n",
      " 6.46266920e-01 2.12738485e-01 2.31986209e-02 2.91185375e-01\n",
      " 8.24246118e-02 3.86766137e-02 1.57656954e-01 8.72015651e-02\n",
      " 3.16189358e-01 6.21504049e-01 1.69368174e-01 2.28733523e-01\n",
      " 2.07120362e-01 4.80423162e-01 1.56929628e-01 6.14548106e-01\n",
      " 2.84329509e-02 1.03277561e-01 5.36909545e-01 8.18389318e-02\n",
      " 1.96647018e-02 2.80994284e-02 1.67460326e-01 1.11507729e-01\n",
      " 2.24253576e-01 1.88075369e-01 2.28918609e-01 6.54475450e-01\n",
      " 8.54865355e-03 1.48352529e-01 2.18362128e-01 3.89059997e-02\n",
      " 2.41331892e-01 8.62865051e-01 3.17314847e-01 2.73128084e-01\n",
      " 3.41337637e-01 3.23686586e-02 6.30354544e-01 1.97831929e-01\n",
      " 6.38148993e-01 1.10132538e-02 1.32886328e-02 1.86275743e-01\n",
      " 2.75815190e-01 3.91373993e-01 6.32486757e-01 7.99523616e-02\n",
      " 5.51245807e-01 3.69421072e-01 2.24091647e-01 3.02296814e-01\n",
      " 5.25326243e-01 3.98819398e-01 6.39806507e-02 1.22391063e-01\n",
      " 3.34695694e-01 4.18134285e-02 1.63879257e-01 5.54304617e-01\n",
      " 2.90054665e-01 5.34751777e-01 9.11133618e-02 5.67166958e-01\n",
      " 1.42750043e-02 2.66727339e-02 1.98010138e-01 8.38003758e-01\n",
      " 7.39300471e-01 4.35160469e-01 1.05947871e-01 1.55186525e-01\n",
      " 5.62318914e-01 3.38348632e-01 2.33353924e-01 4.50303667e-01\n",
      " 4.21286482e-02 6.61457257e-01 2.36306917e-01 5.18377196e-01\n",
      " 3.06042975e-02 1.47530526e-01 2.82485085e-01 1.26937404e-02\n",
      " 8.03725183e-02 3.19832441e-01 2.69104177e-01 4.53822165e-01\n",
      " 7.07122777e-01 4.10206502e-01 1.12111087e-02 6.88765710e-02\n",
      " 2.47071679e-01 4.38627678e-01 8.93807722e-02 6.57043266e-02\n",
      " 3.80015020e-02 4.60744288e-01 3.89035763e-01 1.44026479e-01\n",
      " 4.04589796e-01 5.01940917e-01 3.17207983e-01 3.38205226e-01\n",
      " 3.44211416e-02 3.87180610e-01 7.16521862e-02 3.45953716e-01\n",
      " 3.73201168e-01 6.04566388e-01 1.94994103e-01 9.94166957e-02\n",
      " 5.89133009e-02 7.60633910e-01 2.41200296e-01 4.20128269e-01\n",
      " 1.21505974e-01 1.51109262e-01 6.00071038e-01 8.39263410e-01\n",
      " 6.80899101e-01 1.73296283e-01 3.76931470e-02 9.07378642e-01\n",
      " 1.29288521e-01 1.14267190e-01 2.96225169e-01 3.88417202e-01\n",
      " 2.46169998e-01 1.19693189e-01 3.27914276e-01 1.85135792e-01\n",
      " 9.54046782e-02 1.11646819e-01 1.86914335e-02 4.77315090e-01\n",
      " 7.19948834e-01 8.36840445e-02 1.45815097e-01 4.83602847e-01\n",
      " 2.80927296e-01 1.67601976e-01 1.96340366e-01 5.40752859e-01\n",
      " 5.06061580e-01 4.41909480e-01 5.11176491e-01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8120879813449298"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "training_features = ['seniority', 'income', 'assets', 'records', 'job', 'home']\n",
    "\n",
    "train_dict = train[training_features].to_dict(orient='records')\n",
    "X_train = dv.fit_transform(train_dict)\n",
    "y_train = train[\"default\"]\n",
    "print(X_train)\n",
    "\n",
    "val_dict = val[training_features].to_dict(orient='records')\n",
    "X_val = dv.transform(val_dict)\n",
    "y_val = val[\"default\"]\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "# solver='lbfgs' is the default solver in newer version of sklearn\n",
    "# for older versions, you need to specify it explicitly\n",
    "model.fit(X_train, y_train)\n",
    "y_val_pred = model.predict_proba(X_val)[:,1]\n",
    "print(y_val_pred)\n",
    "roc_auc_score(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "What's the AUC of this model on the validation dataset? (round to 3 digits)\n",
    "\n",
    "- 0.512\n",
    "- 0.612\n",
    "- 0.712\n",
    "- 0.812"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Now let's compute precision and recall for our model.\n",
    "\n",
    "* Evaluate the model on all thresholds from 0.0 to 1.0 with step 0.01\n",
    "* For each threshold, compute precision and recall\n",
    "* Plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13\n",
      " 0.14 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27\n",
      " 0.28 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41\n",
      " 0.42 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55\n",
      " 0.56 0.57 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69\n",
      " 0.7  0.71 0.72 0.73 0.74 0.75 0.76 0.77 0.78 0.79 0.8  0.81 0.82 0.83\n",
      " 0.84 0.85 0.86 0.87 0.88 0.89 0.9  0.91 0.92 0.93 0.94 0.95 0.96 0.97\n",
      " 0.98 0.99 1.  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7s/5h000n6s2z75cbf4y33k39y40000gn/T/ipykernel_56864/3644441012.py:19: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  scores.append((t, tp, fp, fn, tn, (tp / (tp+fp)), (tp / (tp + fn))))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f98f9ce26a0>]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "thresholds = np.linspace(0, 1, 101)\n",
    "print(thresholds)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for t in thresholds:\n",
    "    actual_positive = (y_val == 1)\n",
    "    actual_negative = (y_val == 0)\n",
    "    \n",
    "    predict_positive = (y_val_pred >= t)\n",
    "    predict_negative = (y_val_pred < t)\n",
    "\n",
    "    tp = (predict_positive & actual_positive).sum()\n",
    "    tn = (predict_negative & actual_negative).sum()\n",
    "\n",
    "    fp = (predict_positive & actual_negative).sum()\n",
    "    fn = (predict_negative & actual_positive).sum()\n",
    "    \n",
    "    scores.append((t, tp, fp, fn, tn, (tp / (tp+fp)), (tp / (tp + fn))))\n",
    "    \n",
    "precisions = list(map(lambda x : x[5], scores))\n",
    "recalls = list(map(lambda x : x[6], scores))\n",
    "plt.plot(thresholds, precisions)\n",
    "plt.plot(thresholds, recalls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At which threshold precision and recall curves intersect?\n",
    "\n",
    "* 0.2\n",
    "* 0.4\n",
    "* 0.6\n",
    "* 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Precision and recall are conflicting - when one grows, the other goes down. That's why they are often combined into the F1 score - a metrics that takes into account both\n",
    "\n",
    "This is the formula for computing F1:\n",
    "\n",
    "$$F_1 = 2 \\cdot \\cfrac{P \\cdot R}{P + R}$$\n",
    "\n",
    "Where $P$ is precision and $R$ is recall.\n",
    "\n",
    "Let's compute F1 for all thresholds from 0.0 to 1.0 with increment 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 0.43271767810026385), (0.01, 0.4365572315882875), (0.02, 0.442429737080689), (0.03, 0.4477335800185014), (0.04, 0.46051379638439577), (0.05, 0.46317829457364346), (0.06, 0.47279920870425324), (0.07, 0.4812942366026289), (0.08, 0.4917355371900827), (0.09, 0.5015873015873017), (0.1, 0.5075593952483801), (0.11, 0.5138427464008859), (0.12, 0.5257142857142857), (0.13, 0.5394581861012957), (0.14, 0.5478468899521531), (0.15, 0.562268803945746), (0.16, 0.5652724968314322), (0.17, 0.5699481865284974), (0.18, 0.5778364116094987), (0.19, 0.5879194630872484), (0.2, 0.5906593406593407), (0.21, 0.5952045133991537), (0.22, 0.6054519368723099), (0.23, 0.6072992700729927), (0.24, 0.6107784431137725), (0.25, 0.6137404580152672), (0.26, 0.6108527131782946), (0.27, 0.6117274167987321), (0.28, 0.617124394184168), (0.29, 0.6217105263157895), (0.3, 0.6285714285714286), (0.31, 0.6279863481228668), (0.32, 0.6273830155979202), (0.33, 0.6219081272084805), (0.34, 0.6220614828209765), (0.35000000000000003, 0.6176470588235294), (0.36, 0.6142322097378277), (0.37, 0.6133333333333333), (0.38, 0.6031128404669261), (0.39, 0.5936254980079682), (0.4, 0.5761316872427984), (0.41000000000000003, 0.5726315789473686), (0.42, 0.5671641791044776), (0.43, 0.5572354211663066), (0.44, 0.5582417582417583), (0.45, 0.551111111111111), (0.46, 0.5540540540540541), (0.47000000000000003, 0.547945205479452), (0.48, 0.5450346420323325), (0.49, 0.5399061032863849), (0.5, 0.5377358490566039), (0.51, 0.5288461538461539), (0.52, 0.5134474327628361), (0.53, 0.5123152709359606), (0.54, 0.505050505050505), (0.55, 0.4948453608247423), (0.56, 0.4776902887139108), (0.5700000000000001, 0.4516129032258065), (0.58, 0.44141689373297005), (0.59, 0.4371584699453552), (0.6, 0.430939226519337), (0.61, 0.4124293785310734), (0.62, 0.4034582132564842), (0.63, 0.39766081871345027), (0.64, 0.37237237237237236), (0.65, 0.36085626911314983), (0.66, 0.3302180685358255), (0.67, 0.33125), (0.68, 0.33228840125391845), (0.6900000000000001, 0.3076923076923077), (0.7000000000000001, 0.24749163879598665), (0.71, 0.2424242424242424), (0.72, 0.22525597269624573), (0.73, 0.22525597269624573), (0.74, 0.21305841924398625), (0.75, 0.1958041958041958), (0.76, 0.1958041958041958), (0.77, 0.18505338078291814), (0.78, 0.16546762589928057), (0.79, 0.15942028985507245), (0.8, 0.15441176470588236), (0.81, 0.15441176470588236), (0.8200000000000001, 0.14814814814814817), (0.8300000000000001, 0.1348314606741573), (0.84, 0.09923664122137404), (0.85, 0.08461538461538463), (0.86, 0.08461538461538463), (0.87, 0.0697674418604651), (0.88, 0.06225680933852141), (0.89, 0.054901960784313725), (0.9, 0.04724409448818898), (0.91, 0.03187250996015937), (0.92, 0.03187250996015937), (0.93, 0.01612903225806452), (0.9400000000000001, nan), (0.9500000000000001, nan), (0.96, nan), (0.97, nan), (0.98, nan), (0.99, nan), (1.0, nan)]\n",
      "[(0.93, 0.01612903225806452), (0.91, 0.03187250996015937), (0.92, 0.03187250996015937), (0.9, 0.04724409448818898), (0.89, 0.054901960784313725), (0.88, 0.06225680933852141), (0.87, 0.0697674418604651), (0.85, 0.08461538461538463), (0.86, 0.08461538461538463), (0.84, 0.09923664122137404), (0.8300000000000001, 0.1348314606741573), (0.8200000000000001, 0.14814814814814817), (0.8, 0.15441176470588236), (0.81, 0.15441176470588236), (0.79, 0.15942028985507245), (0.78, 0.16546762589928057), (0.77, 0.18505338078291814), (0.75, 0.1958041958041958), (0.76, 0.1958041958041958), (0.74, 0.21305841924398625), (0.72, 0.22525597269624573), (0.73, 0.22525597269624573), (0.71, 0.2424242424242424), (0.7000000000000001, 0.24749163879598665), (0.6900000000000001, 0.3076923076923077), (0.66, 0.3302180685358255), (0.67, 0.33125), (0.68, 0.33228840125391845), (0.65, 0.36085626911314983), (0.64, 0.37237237237237236), (0.63, 0.39766081871345027), (0.62, 0.4034582132564842), (0.61, 0.4124293785310734), (0.6, 0.430939226519337), (0.0, 0.43271767810026385), (0.01, 0.4365572315882875), (0.59, 0.4371584699453552), (0.58, 0.44141689373297005), (0.02, 0.442429737080689), (0.03, 0.4477335800185014), (0.5700000000000001, 0.4516129032258065), (0.04, 0.46051379638439577), (0.05, 0.46317829457364346), (0.06, 0.47279920870425324), (0.56, 0.4776902887139108), (0.07, 0.4812942366026289), (0.08, 0.4917355371900827), (0.55, 0.4948453608247423), (0.09, 0.5015873015873017), (0.54, 0.505050505050505), (0.1, 0.5075593952483801), (0.53, 0.5123152709359606), (0.52, 0.5134474327628361), (0.11, 0.5138427464008859), (0.12, 0.5257142857142857), (0.51, 0.5288461538461539), (0.5, 0.5377358490566039), (0.13, 0.5394581861012957), (0.49, 0.5399061032863849), (0.48, 0.5450346420323325), (0.14, 0.5478468899521531), (0.47000000000000003, 0.547945205479452), (0.45, 0.551111111111111), (0.46, 0.5540540540540541), (0.43, 0.5572354211663066), (0.44, 0.5582417582417583), (0.15, 0.562268803945746), (0.16, 0.5652724968314322), (0.42, 0.5671641791044776), (0.17, 0.5699481865284974), (0.41000000000000003, 0.5726315789473686), (0.4, 0.5761316872427984), (0.18, 0.5778364116094987), (0.19, 0.5879194630872484), (0.2, 0.5906593406593407), (0.39, 0.5936254980079682), (0.21, 0.5952045133991537), (0.38, 0.6031128404669261), (0.22, 0.6054519368723099), (0.23, 0.6072992700729927), (0.24, 0.6107784431137725), (0.26, 0.6108527131782946), (0.27, 0.6117274167987321), (0.37, 0.6133333333333333), (0.25, 0.6137404580152672), (0.36, 0.6142322097378277), (0.28, 0.617124394184168), (0.35000000000000003, 0.6176470588235294), (0.29, 0.6217105263157895), (0.33, 0.6219081272084805), (0.34, 0.6220614828209765), (0.32, 0.6273830155979202), (0.31, 0.6279863481228668), (0.3, 0.6285714285714286), (0.9400000000000001, nan), (0.9500000000000001, nan), (0.96, nan), (0.97, nan), (0.98, nan), (0.99, nan), (1.0, nan)]\n"
     ]
    }
   ],
   "source": [
    "f1s = []\n",
    "for score in scores:\n",
    "    precision = score[5]\n",
    "    recall = score[6]\n",
    "    f1s.append((score[0], 2*((precision * recall) / (precision + recall))))\n",
    "\n",
    "print(f1s)\n",
    "print(sorted(f1s, key=lambda x:x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At which threshold F1 is maximal?\n",
    "\n",
    "- 0.1\n",
    "- 0.3\n",
    "- 0.5\n",
    "- 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "\n",
    "Use the `KFold` class from Scikit-Learn to evaluate our model on 5 different folds:\n",
    "\n",
    "```\n",
    "KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "```\n",
    "\n",
    "* Iterate over different folds of `df_full_train`\n",
    "* Split the data into train and validation\n",
    "* Train the model on train with these parameters: `LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)`\n",
    "* Use AUC to evaluate the model on validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8256212256212256, 0.8254077031563474, 0.789298525308888, 0.8153421373265731, 0.8054696294614563]\n",
      "0.013669069470482898\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train(df_train, y_train, C=1.0):\n",
    "    dicts = df_train[training_features].to_dict(orient='records')\n",
    "\n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    X_train = dv.fit_transform(dicts)\n",
    "\n",
    "    model = LogisticRegression(solver='liblinear', C=C, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return dv, model\n",
    "\n",
    "def predict(df, dv, model):\n",
    "    dicts = df[training_features].to_dict(orient='records')\n",
    "\n",
    "    X = dv.transform(dicts)\n",
    "    y_pred = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "scores = []\n",
    "\n",
    "for train_idx, val_idx in kfold.split(train_val):\n",
    "    df_train = train_val.iloc[train_idx]\n",
    "    df_val = train_val.iloc[val_idx]\n",
    "\n",
    "    y_train = df_train.default.values\n",
    "    y_val = df_val.default.values\n",
    "\n",
    "    dv, model = train(df_train, y_train, C=1.0)\n",
    "    y_pred = predict(df_val, dv, model)\n",
    "\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    scores.append(auc)\n",
    "    \n",
    "print(scores)\n",
    "print(np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How large is standard devidation of the scores across different folds?\n",
    "\n",
    "- 0.001\n",
    "- 0.014\n",
    "- 0.09\n",
    "- 0.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "Now let's use 5-Fold cross-validation to find the best parameter C\n",
    "\n",
    "* Iterate over the following C values: `[0.01, 0.1, 1, 10]`\n",
    "* Initialize `KFold` with the same parameters as previously\n",
    "* Use these parametes for the model: `LogisticRegression(solver='liblinear', C=C, max_iter=1000)`\n",
    "* Compute the mean score as well as the std (round the mean and std to 3 decimal digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.01 0.809 +- 0.013\n",
      "C=0.1 0.813 +- 0.014\n",
      "C=1 0.812 +- 0.014\n",
      "C=10 0.814 +- 0.015\n"
     ]
    }
   ],
   "source": [
    "C_candidates = [0.01, 0.1, 1, 10]\n",
    "for C in C_candidates:\n",
    "        kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "        scores = []\n",
    "\n",
    "        for train_idx, val_idx in kfold.split(train_val):\n",
    "            df_train = train_val.iloc[train_idx]\n",
    "            df_val = train_val.iloc[val_idx]\n",
    "\n",
    "            y_train = df_train.default.values\n",
    "            y_val = df_val.default.values\n",
    "\n",
    "            dv, model = train(df_train, y_train, C=C)\n",
    "            y_pred = predict(df_val, dv, model)\n",
    "\n",
    "            auc = roc_auc_score(y_val, y_pred)\n",
    "            scores.append(auc)\n",
    "            \n",
    "        print('C=%s %.3f +- %.3f' % (C, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which C leads to the best mean score?\n",
    "\n",
    "- 0.01\n",
    "- 0.1\n",
    "- 1\n",
    "- 10\n",
    "\n",
    "If you have ties, select the score with the lowest std. If you still have ties, select the smallest C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the results\n",
    "\n",
    "Submit your results here: https://forms.gle/e497sR5iB36mM9Cs5\n",
    "\n",
    "It's possible that your answers won't match exactly. If it's the case, select the closest one.\n",
    "\n",
    "## Deadline\n",
    "\n",
    "The deadline for submitting is 04 October 2021, 17:00 CET. After that, the form will be closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
